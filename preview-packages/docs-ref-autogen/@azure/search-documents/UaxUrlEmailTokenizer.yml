### YamlMime:TSType
name: UaxUrlEmailTokenizer
uid: '@azure/search-documents.UaxUrlEmailTokenizer'
package: '@azure/search-documents'
summary: >-
  Tokenizes urls and emails as one token. This tokenizer is implemented using
  Apache Lucene.
fullName: UaxUrlEmailTokenizer
remarks: ''
isDeprecated: false
type: interface
properties:
  - name: maxTokenLength
    uid: '@azure/search-documents.UaxUrlEmailTokenizer.maxTokenLength'
    package: '@azure/search-documents'
    summary: >-
      The maximum token length. Default is 255. Tokens longer than the maximum
      length are split. The maximum token length that can be used is 300
      characters.
    fullName: maxTokenLength
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'maxTokenLength?: number'
      return:
        description: ''
        type: number
  - name: odatatype
    uid: '@azure/search-documents.UaxUrlEmailTokenizer.odatatype'
    package: '@azure/search-documents'
    summary: >-
      Polymorphic discriminator, which specifies the different types this object
      can be
    fullName: odatatype
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'odatatype: "#Microsoft.Azure.Search.UaxUrlEmailTokenizer"'
      return:
        description: ''
        type: '"#<xref uid="Microsoft.Azure.Search.UaxUrlEmailTokenizer" />"'
inheritedProperties:
  - name: name
    uid: '@azure/search-documents.UaxUrlEmailTokenizer.name'
    package: '@azure/search-documents'
    summary: >-
      The name of the tokenizer. It must only contain letters, digits, spaces,
      dashes or underscores, can only start and end with alphanumeric
      characters, and is limited to 128 characters.
    fullName: name
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'name: string'
      return:
        description: ''
        type: string
    inheritanceDescription: >-
      <b>Inherited From</b>
      [LexicalTokenizer.name](xref:@azure/search-documents.BaseLexicalTokenizer.name)
extends: <xref uid="@azure/search-documents.BaseLexicalTokenizer" />

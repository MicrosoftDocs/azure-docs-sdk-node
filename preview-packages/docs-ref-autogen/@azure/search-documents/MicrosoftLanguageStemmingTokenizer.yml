### YamlMime:TSType
name: MicrosoftLanguageStemmingTokenizer
uid: '@azure/search-documents.MicrosoftLanguageStemmingTokenizer'
package: '@azure/search-documents'
summary: >-
  Divides text using language-specific rules and reduces words to their base
  forms.
fullName: MicrosoftLanguageStemmingTokenizer
remarks: ''
isPreview: false
isDeprecated: false
type: interface
properties:
  - name: isSearchTokenizer
    uid: >-
      @azure/search-documents.MicrosoftLanguageStemmingTokenizer.isSearchTokenizer
    package: '@azure/search-documents'
    summary: >-
      A value indicating how the tokenizer is used. Set to true if used as the
      search tokenizer, set

      to false if used as the indexing tokenizer. Default is false. Default
      value: false.
    fullName: isSearchTokenizer
    remarks: ''
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'isSearchTokenizer?: boolean'
      return:
        type: boolean
        description: ''
  - name: language
    uid: '@azure/search-documents.MicrosoftLanguageStemmingTokenizer.language'
    package: '@azure/search-documents'
    summary: >-
      The language to use. The default is English. Possible values include:
      'Arabic', 'Bangla',

      'Bulgarian', 'Catalan', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English',
      'Estonian',

      'Finnish', 'French', 'German', 'Greek', 'Gujarati', 'Hebrew', 'Hindi',
      'Hungarian',

      'Icelandic', 'Indonesian', 'Italian', 'Kannada', 'Latvian', 'Lithuanian',
      'Malay',

      'Malayalam', 'Marathi', 'NorwegianBokmaal', 'Polish', 'Portuguese',
      'PortugueseBrazilian',

      'Punjabi', 'Romanian', 'Russian', 'SerbianCyrillic', 'SerbianLatin',
      'Slovak', 'Slovenian',

      'Spanish', 'Swedish', 'Tamil', 'Telugu', 'Turkish', 'Ukrainian', 'Urdu'
    fullName: language
    remarks: ''
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'language?: MicrosoftStemmingTokenizerLanguage'
      return:
        type: >-
          <xref uid="@azure/search-documents.MicrosoftStemmingTokenizerLanguage"
          />
        description: ''
  - name: maxTokenLength
    uid: '@azure/search-documents.MicrosoftLanguageStemmingTokenizer.maxTokenLength'
    package: '@azure/search-documents'
    summary: >-
      The maximum token length. Tokens longer than the maximum length are split.
      Maximum token

      length that can be used is 300 characters. Tokens longer than 300
      characters are first split

      into tokens of length 300 and then each of those tokens is split based on
      the max token length

      set. Default is 255. Default value: 255.
    fullName: maxTokenLength
    remarks: ''
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'maxTokenLength?: number'
      return:
        type: number
        description: ''
  - name: name
    uid: '@azure/search-documents.MicrosoftLanguageStemmingTokenizer.name'
    package: '@azure/search-documents'
    summary: >-
      The name of the tokenizer. It must only contain letters, digits, spaces,
      dashes or

      underscores, can only start and end with alphanumeric characters, and is
      limited to 128

      characters.
    fullName: name
    remarks: ''
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'name: string'
      return:
        type: string
        description: ''
  - name: odatatype
    uid: '@azure/search-documents.MicrosoftLanguageStemmingTokenizer.odatatype'
    package: '@azure/search-documents'
    summary: Polymorphic Discriminator
    fullName: odatatype
    remarks: ''
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'odatatype: "#Microsoft.Azure.Search.MicrosoftLanguageStemmingTokenizer"'
      return:
        type: >-
          "#<xref
          uid="Microsoft.Azure.Search.MicrosoftLanguageStemmingTokenizer" />"
        description: ''

### YamlMime:TSType
name: NGramTokenizer
uid: '@azure/search-documents.NGramTokenizer'
package: '@azure/search-documents'
summary: >-
  Tokenizes the input into n-grams of the given size(s). This tokenizer is
  implemented using

  Apache Lucene.
fullName: NGramTokenizer
remarks: ''
isPreview: false
isDeprecated: false
type: interface
properties:
  - name: maxGram
    uid: '@azure/search-documents.NGramTokenizer.maxGram'
    package: '@azure/search-documents'
    summary: 'The maximum n-gram length. Default is 2. Maximum is 300. Default value: 2.'
    fullName: maxGram
    remarks: ''
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'maxGram?: number'
      return:
        type: number
        description: ''
  - name: minGram
    uid: '@azure/search-documents.NGramTokenizer.minGram'
    package: '@azure/search-documents'
    summary: >-
      The minimum n-gram length. Default is 1. Maximum is 300. Must be less than
      the value of

      maxGram. Default value: 1.
    fullName: minGram
    remarks: ''
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'minGram?: number'
      return:
        type: number
        description: ''
  - name: name
    uid: '@azure/search-documents.NGramTokenizer.name'
    package: '@azure/search-documents'
    summary: >-
      The name of the tokenizer. It must only contain letters, digits, spaces,
      dashes or

      underscores, can only start and end with alphanumeric characters, and is
      limited to 128

      characters.
    fullName: name
    remarks: ''
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'name: string'
      return:
        type: string
        description: ''
  - name: odatatype
    uid: '@azure/search-documents.NGramTokenizer.odatatype'
    package: '@azure/search-documents'
    summary: Polymorphic Discriminator
    fullName: odatatype
    remarks: ''
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'odatatype: "#Microsoft.Azure.Search.NGramTokenizer"'
      return:
        type: '"#<xref uid="Microsoft.Azure.Search.NGramTokenizer" />"'
        description: ''
  - name: tokenChars
    uid: '@azure/search-documents.NGramTokenizer.tokenChars'
    package: '@azure/search-documents'
    summary: Character classes to keep in the tokens.
    fullName: tokenChars
    remarks: ''
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'tokenChars?: TokenCharacterKind[]'
      return:
        type: '<xref uid="@azure/search-documents.TokenCharacterKind" />[]'
        description: ''

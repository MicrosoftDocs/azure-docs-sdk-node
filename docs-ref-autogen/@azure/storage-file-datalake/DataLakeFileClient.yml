### YamlMime:TSType
name: DataLakeFileClient
uid: '@azure/storage-file-datalake.DataLakeFileClient'
package: '@azure/storage-file-datalake'
summary: A DataLakeFileClient represents a URL to the Azure Storage file.
fullName: DataLakeFileClient
remarks: ''
isDeprecated: false
type: class
constructors:
  - name: DataLakeFileClient(string, Pipeline)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.constructor_1'
    package: '@azure/storage-file-datalake'
    summary: Creates an instance of DataLakeFileClient from url and pipeline.
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'new DataLakeFileClient(url: string, pipeline: Pipeline)'
      parameters:
        - id: url
          type: string
          description: |-
            A Client string pointing to Azure Storage data lake file, such as
                                "https://myaccount.dfs.core.windows.net/filesystem/file".
                                You can append a SAS if using AnonymousCredential, such as "https://myaccount.dfs.core.windows.net/filesystem/directory/file?sasString".
        - id: pipeline
          type: <xref uid="@azure/storage-file-datalake.Pipeline" />
          description: |-
            Call newPipeline() to create a default
                                       pipeline, or provide a customized pipeline.
  - name: >-
      DataLakeFileClient(string, StorageSharedKeyCredential |
      AnonymousCredential | TokenCredential, StoragePipelineOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.constructor'
    package: '@azure/storage-file-datalake'
    summary: Creates an instance of DataLakeFileClient from url and credential.
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        new DataLakeFileClient(url: string, credential?:
        StorageSharedKeyCredential | AnonymousCredential | TokenCredential,
        options?: StoragePipelineOptions)
      parameters:
        - id: url
          type: string
          description: |-
            A Client string pointing to Azure Storage data lake file, such as
                                "https://myaccount.dfs.core.windows.net/filesystem/file".
                                You can append a SAS if using AnonymousCredential, such as "https://myaccount.dfs.core.windows.net/filesystem/directory/file?sasString".
        - id: credential
          type: >-
            <xref uid="@azure/storage-file-datalake.StorageSharedKeyCredential"
            /> | <xref uid="@azure/storage-file-datalake.AnonymousCredential" />
            | <xref uid="@azure/core-auth.TokenCredential" />
          description: >-
            Such as AnonymousCredential, StorageSharedKeyCredential or any
            credential from the `@azure/identity` package to authenticate
            requests to the service. You can also provide an object that
            implements the TokenCredential interface. If not specified,
            AnonymousCredential is used.
        - id: options
          type: <xref uid="@azure/storage-file-datalake.StoragePipelineOptions" />
          description: Optional. Options to configure the HTTP pipeline.
properties:
  - name: fileSystemName
    uid: '@azure/storage-file-datalake.DataLakeFileClient.fileSystemName'
    package: '@azure/storage-file-datalake'
    summary: Name of current file system.
    fullName: fileSystemName
    remarks: ''
    isDeprecated: false
    syntax:
      content: string fileSystemName
      return:
        description: ''
        type: string
  - name: name
    uid: '@azure/storage-file-datalake.DataLakeFileClient.name'
    package: '@azure/storage-file-datalake'
    summary: Name of current path (directory or file).
    fullName: name
    remarks: ''
    isDeprecated: false
    syntax:
      content: string name
      return:
        description: ''
        type: string
inheritedProperties:
  - name: accountName
    uid: '@azure/storage-file-datalake.DataLakeFileClient.accountName'
    package: '@azure/storage-file-datalake'
    summary: ''
    fullName: accountName
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'accountName: string'
      return:
        description: ''
        type: string
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.accountName](xref:@azure/storage-file-datalake.DataLakePathClient.accountName)
  - name: credential
    uid: '@azure/storage-file-datalake.DataLakeFileClient.credential'
    package: '@azure/storage-file-datalake'
    summary: >-
      Such as AnonymousCredential, StorageSharedKeyCredential or any credential
      from the `@azure/identity` package to authenticate requests to the
      service. You can also provide an object that implements the
      TokenCredential interface. If not specified, AnonymousCredential is used.
    fullName: credential
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        credential: StorageSharedKeyCredential | AnonymousCredential |
        TokenCredential
      return:
        description: ''
        type: >-
          <xref uid="@azure/storage-file-datalake.StorageSharedKeyCredential" />
          | <xref uid="@azure/storage-file-datalake.AnonymousCredential" /> |
          <xref uid="@azure/core-auth.TokenCredential" />
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.credential](xref:@azure/storage-file-datalake.DataLakePathClient.credential)
  - name: url
    uid: '@azure/storage-file-datalake.DataLakeFileClient.url'
    package: '@azure/storage-file-datalake'
    summary: Encoded URL string value.
    fullName: url
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'url: string'
      return:
        description: ''
        type: string
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.url](xref:@azure/storage-file-datalake.DataLakePathClient.url)
inheritedMethods:
  - name: delete(boolean, PathDeleteOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.delete'
    package: '@azure/storage-file-datalake'
    summary: >-
      Delete current path (directory or file).


      See
      https://learn.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/delete
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function delete(recursive?: boolean, options?: PathDeleteOptions):
        Promise<PathDeleteResponse>
      parameters:
        - id: recursive
          type: boolean
          description: >-
            Required and valid only when the resource is a directory. If "true",
            all paths beneath the directory will be deleted.
        - id: options
          type: <xref uid="@azure/storage-file-datalake.PathDeleteOptions" />
          description: Optional. Options when deleting path.
      return:
        description: ''
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.PathDeleteResponse"
          />&gt;
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.delete](xref:@azure/storage-file-datalake.DataLakePathClient.delete)
  - name: deleteIfExists(boolean, PathDeleteOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.deleteIfExists'
    package: '@azure/storage-file-datalake'
    summary: >-
      Delete current path (directory or file) if it exists.


      See
      https://learn.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/delete
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function deleteIfExists(recursive?: boolean, options?:
        PathDeleteOptions): Promise<PathDeleteIfExistsResponse>
      parameters:
        - id: recursive
          type: boolean
          description: >-
            Required and valid only when the resource is a directory. If "true",
            all paths beneath the directory will be deleted.
        - id: options
          type: <xref uid="@azure/storage-file-datalake.PathDeleteOptions" />
          description: ''
      return:
        description: ''
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathDeleteIfExistsResponse" />&gt;
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.deleteIfExists](xref:@azure/storage-file-datalake.DataLakePathClient.deleteIfExists)
  - name: exists(PathExistsOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.exists'
    package: '@azure/storage-file-datalake'
    summary: >-
      Returns true if the Data Lake file represented by this client exists;
      false otherwise.


      NOTE: use this function with care since an existing file might be deleted
      by other clients or

      applications. Vice versa new files might be added by other clients or
      applications after this

      function completes.
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'function exists(options?: PathExistsOptions): Promise<boolean>'
      parameters:
        - id: options
          type: <xref uid="@azure/storage-file-datalake.PathExistsOptions" />
          description: options to Exists operation.
      return:
        description: ''
        type: Promise&lt;boolean&gt;
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.exists](xref:@azure/storage-file-datalake.DataLakePathClient.exists)
  - name: getAccessControl(PathGetAccessControlOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.getAccessControl'
    package: '@azure/storage-file-datalake'
    summary: >-
      Returns the access control data for a path (directory of file).


      See
      https://learn.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/getproperties
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function getAccessControl(options?: PathGetAccessControlOptions):
        Promise<PathGetAccessControlResponse>
      parameters:
        - id: options
          type: >-
            <xref uid="@azure/storage-file-datalake.PathGetAccessControlOptions"
            />
          description: Optional. Options when getting file access control.
      return:
        description: ''
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathGetAccessControlResponse" />&gt;
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.getAccessControl](xref:@azure/storage-file-datalake.DataLakePathClient.getAccessControl)
  - name: getDataLakeLeaseClient(string)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.getDataLakeLeaseClient'
    package: '@azure/storage-file-datalake'
    summary: >-
      Get a
      [DataLakeLeaseClient](xref:@azure/storage-file-datalake.DataLakeLeaseClient)
      that manages leases on the path (directory or file).
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function getDataLakeLeaseClient(proposeLeaseId?: string):
        DataLakeLeaseClient
      parameters:
        - id: proposeLeaseId
          type: string
          description: Optional. Initial proposed lease Id.
      return:
        description: ''
        type: <xref uid="@azure/storage-file-datalake.DataLakeLeaseClient" />
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.getDataLakeLeaseClient](xref:@azure/storage-file-datalake.DataLakePathClient.getDataLakeLeaseClient)
  - name: getProperties(PathGetPropertiesOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.getProperties'
    package: '@azure/storage-file-datalake'
    summary: >-
      Returns all user-defined metadata, standard HTTP properties, and system
      properties

      for the path (directory or file).


      WARNING: The `metadata` object returned in the response will have its keys
      in lowercase, even if

      they originally contained uppercase characters. This differs from the
      metadata keys returned by

      the methods of
      [DataLakeFileSystemClient](xref:@azure/storage-file-datalake.DataLakeFileSystemClient)
      that list paths using the `includeMetadata` option, which

      will retain their original casing.


      See
      https://learn.microsoft.com/rest/api/storageservices/get-blob-properties
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function getProperties(options?: PathGetPropertiesOptions):
        Promise<PathGetPropertiesResponse>
      parameters:
        - id: options
          type: <xref uid="@azure/storage-file-datalake.PathGetPropertiesOptions" />
          description: Optional. Options when getting path properties.
      return:
        description: ''
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathGetPropertiesResponse" />&gt;
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.getProperties](xref:@azure/storage-file-datalake.DataLakePathClient.getProperties)
  - name: move(string, PathMoveOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.move'
    package: '@azure/storage-file-datalake'
    summary: >-
      Move directory or file within same file system.


      See
      https://learn.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/create
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function move(destinationPath: string, options?: PathMoveOptions):
        Promise<PathMoveResponse>
      parameters:
        - id: destinationPath
          type: string
          description: >-
            Destination directory path like "directory" or file path
            "directory/file".
                                            If the destinationPath is authenticated with SAS, add the SAS to the destination path like "directory/file?sasToken".
        - id: options
          type: <xref uid="@azure/storage-file-datalake.PathMoveOptions" />
          description: Optional. Options when moving directory or file.
      return:
        description: ''
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.PathMoveResponse"
          />&gt;
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.move](xref:@azure/storage-file-datalake.DataLakePathClient.move)
  - name: move(string, string, PathMoveOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.move_1'
    package: '@azure/storage-file-datalake'
    summary: >-
      Move directory or file to another file system.


      See
      https://learn.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/create
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function move(destinationFileSystem: string, destinationPath: string,
        options?: PathMoveOptions): Promise<PathMoveResponse>
      parameters:
        - id: destinationFileSystem
          type: string
          description: Destination file system like "filesystem".
        - id: destinationPath
          type: string
          description: >-
            Destination directory path like "directory" or file path
            "directory/file"
                                            If the destinationPath is authenticated with SAS, add the SAS to the destination path like "directory/file?sasToken".
        - id: options
          type: <xref uid="@azure/storage-file-datalake.PathMoveOptions" />
          description: Optional. Options when moving directory or file.
      return:
        description: ''
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.PathMoveResponse"
          />&gt;
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.move](xref:@azure/storage-file-datalake.DataLakePathClient.move)
  - name: >-
      removeAccessControlRecursive(RemovePathAccessControlItem[],
      PathChangeAccessControlRecursiveOptions)
    uid: >-
      @azure/storage-file-datalake.DataLakeFileClient.removeAccessControlRecursive
    package: '@azure/storage-file-datalake'
    summary: >-
      Removes the Access Control on a path and sub paths.


      See
      https://learn.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/update
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function removeAccessControlRecursive(acl:
        RemovePathAccessControlItem[], options?:
        PathChangeAccessControlRecursiveOptions):
        Promise<PathChangeAccessControlRecursiveResponse>
      parameters:
        - id: acl
          type: >-
            <xref uid="@azure/storage-file-datalake.RemovePathAccessControlItem"
            />[]
          description: The POSIX access control list for the file or directory.
        - id: options
          type: >-
            <xref
            uid="@azure/storage-file-datalake.PathChangeAccessControlRecursiveOptions"
            />
          description: Optional. Options
      return:
        description: ''
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathChangeAccessControlRecursiveResponse"
          />&gt;
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.removeAccessControlRecursive](xref:@azure/storage-file-datalake.DataLakePathClient.removeAccessControlRecursive)
  - name: setAccessControl(PathAccessControlItem[], PathSetAccessControlOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.setAccessControl'
    package: '@azure/storage-file-datalake'
    summary: >-
      Set the access control data for a path (directory of file).


      See
      https://learn.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/update
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function setAccessControl(acl: PathAccessControlItem[], options?:
        PathSetAccessControlOptions): Promise<PathSetAccessControlResponse>
      parameters:
        - id: acl
          type: <xref uid="@azure/storage-file-datalake.PathAccessControlItem" />[]
          description: The POSIX access control list for the file or directory.
        - id: options
          type: >-
            <xref uid="@azure/storage-file-datalake.PathSetAccessControlOptions"
            />
          description: Optional. Options when setting path access control.
      return:
        description: ''
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathSetAccessControlResponse" />&gt;
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.setAccessControl](xref:@azure/storage-file-datalake.DataLakePathClient.setAccessControl)
  - name: >-
      setAccessControlRecursive(PathAccessControlItem[],
      PathChangeAccessControlRecursiveOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.setAccessControlRecursive'
    package: '@azure/storage-file-datalake'
    summary: >-
      Sets the Access Control on a path and sub paths.


      See
      https://learn.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/update
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function setAccessControlRecursive(acl: PathAccessControlItem[],
        options?: PathChangeAccessControlRecursiveOptions):
        Promise<PathChangeAccessControlRecursiveResponse>
      parameters:
        - id: acl
          type: <xref uid="@azure/storage-file-datalake.PathAccessControlItem" />[]
          description: The POSIX access control list for the file or directory.
        - id: options
          type: >-
            <xref
            uid="@azure/storage-file-datalake.PathChangeAccessControlRecursiveOptions"
            />
          description: Optional. Options
      return:
        description: ''
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathChangeAccessControlRecursiveResponse"
          />&gt;
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.setAccessControlRecursive](xref:@azure/storage-file-datalake.DataLakePathClient.setAccessControlRecursive)
  - name: setHttpHeaders(PathHttpHeaders, PathSetHttpHeadersOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.setHttpHeaders'
    package: '@azure/storage-file-datalake'
    summary: >-
      Sets system properties on the path (directory or file).


      If no value provided, or no value provided for the specified blob HTTP
      headers,

      these blob HTTP headers without a value will be cleared.


      See
      https://learn.microsoft.com/rest/api/storageservices/set-blob-properties
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function setHttpHeaders(httpHeaders: PathHttpHeaders, options?:
        PathSetHttpHeadersOptions): Promise<PathSetHttpHeadersResponse>
      parameters:
        - id: httpHeaders
          type: <xref uid="@azure/storage-file-datalake.PathHttpHeaders" />
          description: ''
        - id: options
          type: >-
            <xref uid="@azure/storage-file-datalake.PathSetHttpHeadersOptions"
            />
          description: ''
      return:
        description: ''
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathSetHttpHeadersResponse" />&gt;
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.setHttpHeaders](xref:@azure/storage-file-datalake.DataLakePathClient.setHttpHeaders)
  - name: setMetadata(Metadata, PathSetMetadataOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.setMetadata'
    package: '@azure/storage-file-datalake'
    summary: >-
      Sets user-defined metadata for the specified path (directory of file) as
      one or more name-value pairs.


      If no option provided, or no metadata defined in the parameter, the path

      metadata will be removed.


      See https://learn.microsoft.com/rest/api/storageservices/set-blob-metadata
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function setMetadata(metadata?: Metadata, options?:
        PathSetMetadataOptions): Promise<PathSetMetadataResponse>
      parameters:
        - id: metadata
          type: <xref uid="@azure/storage-file-datalake.Metadata" />
          description: |-
            Optional. Replace existing metadata with this value.
                                         If no value provided the existing metadata will be removed.
        - id: options
          type: <xref uid="@azure/storage-file-datalake.PathSetMetadataOptions" />
          description: Optional. Options when setting path metadata.
      return:
        description: ''
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathSetMetadataResponse" />&gt;
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.setMetadata](xref:@azure/storage-file-datalake.DataLakePathClient.setMetadata)
  - name: setPermissions(PathPermissions, PathSetPermissionsOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.setPermissions'
    package: '@azure/storage-file-datalake'
    summary: >-
      Sets the file permissions on a path.


      See
      https://learn.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/update
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function setPermissions(permissions: PathPermissions, options?:
        PathSetPermissionsOptions): Promise<PathSetPermissionsResponse>
      parameters:
        - id: permissions
          type: <xref uid="@azure/storage-file-datalake.PathPermissions" />
          description: >-
            The POSIX access permissions for the file owner, the file owning
            group, and others.
        - id: options
          type: >-
            <xref uid="@azure/storage-file-datalake.PathSetPermissionsOptions"
            />
          description: Optional. Options when setting path permissions.
      return:
        description: ''
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathSetPermissionsResponse" />&gt;
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.setPermissions](xref:@azure/storage-file-datalake.DataLakePathClient.setPermissions)
  - name: toDirectoryClient()
    uid: '@azure/storage-file-datalake.DataLakeFileClient.toDirectoryClient'
    package: '@azure/storage-file-datalake'
    summary: >-
      Convert current DataLakePathClient to DataLakeDirectoryClient if current
      path is a directory.
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'function toDirectoryClient(): DataLakeDirectoryClient'
      return:
        description: ''
        type: <xref uid="@azure/storage-file-datalake.DataLakeDirectoryClient" />
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.toDirectoryClient](xref:@azure/storage-file-datalake.DataLakePathClient.toDirectoryClient)
  - name: toFileClient()
    uid: '@azure/storage-file-datalake.DataLakeFileClient.toFileClient'
    package: '@azure/storage-file-datalake'
    summary: >-
      Convert current DataLakePathClient to DataLakeFileClient if current path
      is a file.
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'function toFileClient(): DataLakeFileClient'
      return:
        description: ''
        type: <xref uid="@azure/storage-file-datalake.DataLakeFileClient" />
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.toFileClient](xref:@azure/storage-file-datalake.DataLakePathClient.toFileClient)
  - name: >-
      updateAccessControlRecursive(PathAccessControlItem[],
      PathChangeAccessControlRecursiveOptions)
    uid: >-
      @azure/storage-file-datalake.DataLakeFileClient.updateAccessControlRecursive
    package: '@azure/storage-file-datalake'
    summary: >-
      Modifies the Access Control on a path and sub paths.


      See
      https://learn.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/update
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function updateAccessControlRecursive(acl: PathAccessControlItem[],
        options?: PathChangeAccessControlRecursiveOptions):
        Promise<PathChangeAccessControlRecursiveResponse>
      parameters:
        - id: acl
          type: <xref uid="@azure/storage-file-datalake.PathAccessControlItem" />[]
          description: The POSIX access control list for the file or directory.
        - id: options
          type: >-
            <xref
            uid="@azure/storage-file-datalake.PathChangeAccessControlRecursiveOptions"
            />
          description: Optional. Options
      return:
        description: ''
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathChangeAccessControlRecursiveResponse"
          />&gt;
    inheritanceDescription: >-
      <b>Inherited From</b>
      [DataLakePathClient.updateAccessControlRecursive](xref:@azure/storage-file-datalake.DataLakePathClient.updateAccessControlRecursive)
methods:
  - name: append(RequestBodyType, number, number, FileAppendOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.append'
    package: '@azure/storage-file-datalake'
    summary: >-
      Uploads data to be appended to a file. Data can only be appended to a
      file.

      To apply perviously uploaded data to a file, call flush.


      See
      https://learn.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/update
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function append(body: RequestBodyType, offset: number, length: number,
        options?: FileAppendOptions): Promise<FileAppendResponse>
      parameters:
        - id: body
          type: <xref uid="@azure/storage-file-datalake.HttpRequestBody" />
          description: Content to be uploaded.
        - id: offset
          type: number
          description: Append offset in bytes.
        - id: length
          type: number
          description: Length of content to append in bytes.
        - id: options
          type: <xref uid="@azure/storage-file-datalake.FileAppendOptions" />
          description: Optional. Options when appending data.
      return:
        description: ''
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.FileAppendResponse"
          />&gt;
  - name: create(FileCreateOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.create_1'
    package: '@azure/storage-file-datalake'
    summary: >-
      Create a file.


      See
      https://learn.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/create
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function create(options?: FileCreateOptions):
        Promise<FileCreateResponse>
      parameters:
        - id: options
          type: <xref uid="@azure/storage-file-datalake.FileCreateOptions" />
          description: Optional. Options when creating file.
      return:
        description: ''
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.FileCreateResponse"
          />&gt;
  - name: create(PathResourceType, PathCreateOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.create'
    package: '@azure/storage-file-datalake'
    summary: >-
      Create a file.


      See
      https://learn.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/create
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function create(resourceType: PathResourceType, options?:
        PathCreateOptions): Promise<PathCreateResponse>
      parameters:
        - id: resourceType
          type: <xref uid="@azure/storage-file-datalake.PathResourceTypeModel" />
          description: Resource type, must be "file" for DataLakeFileClient.
        - id: options
          type: <xref uid="@azure/storage-file-datalake.PathCreateOptions" />
          description: Optional. Options when creating file.
      return:
        description: ''
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.PathCreateResponse"
          />&gt;
  - name: createIfNotExists(FileCreateIfNotExistsOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.createIfNotExists_1'
    package: '@azure/storage-file-datalake'
    summary: >-
      Create a file if it doesn't already exists.


      See
      https://learn.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/create
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function createIfNotExists(options?: FileCreateIfNotExistsOptions):
        Promise<FileCreateIfNotExistsResponse>
      parameters:
        - id: options
          type: >-
            <xref
            uid="@azure/storage-file-datalake.FileCreateIfNotExistsOptions" />
          description: Optional. Options when creating file.
      return:
        description: ''
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.FileCreateIfNotExistsResponse"
          />&gt;
  - name: createIfNotExists(PathResourceType, PathCreateIfNotExistsOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.createIfNotExists'
    package: '@azure/storage-file-datalake'
    summary: >-
      Create a file if it doesn't already exists.


      See
      https://learn.microsoft.com/rest/api/storageservices/datalakestoragegen2/path/create
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function createIfNotExists(resourceType: PathResourceType, options?:
        PathCreateIfNotExistsOptions): Promise<PathCreateIfNotExistsResponse>
      parameters:
        - id: resourceType
          type: <xref uid="@azure/storage-file-datalake.PathResourceTypeModel" />
          description: Resource type, must be "file" for DataLakeFileClient.
        - id: options
          type: >-
            <xref
            uid="@azure/storage-file-datalake.PathCreateIfNotExistsOptions" />
          description: ''
      return:
        description: ''
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathCreateIfNotExistsResponse"
          />&gt;
  - name: flush(number, FileFlushOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.flush'
    package: '@azure/storage-file-datalake'
    summary: Flushes (writes) previously appended data to a file.
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function flush(position: number, options?: FileFlushOptions):
        Promise<FileFlushResponse>
      parameters:
        - id: position
          type: number
          description: |-
            File position to flush.
                                     This parameter allows the caller to upload data in parallel and control the order in which it is appended to the file.
                                     It is required when uploading data to be appended to the file and when flushing previously uploaded data to the file.
                                     The value must be the position where the data is to be appended. Uploaded data is not immediately flushed, or written,
                                     to the file. To flush, the previously uploaded data must be contiguous, the position parameter must be specified and
                                     equal to the length of the file after all data has been written, and there must not be a request entity body included
                                     with the request.
        - id: options
          type: <xref uid="@azure/storage-file-datalake.FileFlushOptions" />
          description: Optional. Options when flushing data.
      return:
        description: ''
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.FileFlushResponse"
          />&gt;
  - name: generateSasStringToSign(FileGenerateSasUrlOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.generateSasStringToSign'
    package: '@azure/storage-file-datalake'
    summary: >-
      Only available for clients constructed with a shared key credential.


      Generates string to sign for a Service Shared Access Signature (SAS) URI
      based on the client properties

      and parameters passed in. The SAS is signed by the shared key credential
      of the client.


      See
      https://learn.microsoft.com/rest/api/storageservices/constructing-a-service-sas
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function generateSasStringToSign(options: FileGenerateSasUrlOptions):
        string
      parameters:
        - id: options
          type: >-
            <xref uid="@azure/storage-file-datalake.FileGenerateSasUrlOptions"
            />
          description: Optional parameters.
      return:
        description: >-
          The SAS URI consisting of the URI to the resource represented by this
          client, followed by the generated SAS token.
        type: string
  - name: generateSasUrl(FileGenerateSasUrlOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.generateSasUrl'
    package: '@azure/storage-file-datalake'
    summary: >-
      Only available for clients constructed with a shared key credential.


      Generates a Service Shared Access Signature (SAS) URI based on the client
      properties

      and parameters passed in. The SAS is signed by the shared key credential
      of the client.


      See
      https://learn.microsoft.com/rest/api/storageservices/constructing-a-service-sas
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function generateSasUrl(options: FileGenerateSasUrlOptions):
        Promise<string>
      parameters:
        - id: options
          type: >-
            <xref uid="@azure/storage-file-datalake.FileGenerateSasUrlOptions"
            />
          description: Optional parameters.
      return:
        description: >-
          The SAS URI consisting of the URI to the resource represented by this
          client, followed by the generated SAS token.
        type: Promise&lt;string&gt;
  - name: >-
      generateUserDelegationSasStringToSign(FileGenerateSasUrlOptions,
      UserDelegationKey)
    uid: >-
      @azure/storage-file-datalake.DataLakeFileClient.generateUserDelegationSasStringToSign
    package: '@azure/storage-file-datalake'
    summary: >-
      Generates string to sign for a Service Shared Access Signature (SAS) URI
      based on the client properties

      and parameters passed in. The SAS is signed by the input user delegation
      key.


      See
      https://learn.microsoft.com/rest/api/storageservices/constructing-a-service-sas
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function generateUserDelegationSasStringToSign(options:
        FileGenerateSasUrlOptions, userDelegationKey: UserDelegationKey): string
      parameters:
        - id: options
          type: >-
            <xref uid="@azure/storage-file-datalake.FileGenerateSasUrlOptions"
            />
          description: Optional parameters.
        - id: userDelegationKey
          type: <xref uid="@azure/storage-file-datalake.UserDelegationKey" />
          description: Return value of `blobServiceClient.getUserDelegationKey()`
      return:
        description: >-
          The SAS URI consisting of the URI to the resource represented by this
          client, followed by the generated SAS token.
        type: string
  - name: generateUserDelegationSasUrl(FileGenerateSasUrlOptions, UserDelegationKey)
    uid: >-
      @azure/storage-file-datalake.DataLakeFileClient.generateUserDelegationSasUrl
    package: '@azure/storage-file-datalake'
    summary: >-
      Generates a Service Shared Access Signature (SAS) URI based on the client
      properties

      and parameters passed in. The SAS is signed by the input user delegation
      key.


      See
      https://learn.microsoft.com/rest/api/storageservices/constructing-a-service-sas
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function generateUserDelegationSasUrl(options:
        FileGenerateSasUrlOptions, userDelegationKey: UserDelegationKey):
        Promise<string>
      parameters:
        - id: options
          type: >-
            <xref uid="@azure/storage-file-datalake.FileGenerateSasUrlOptions"
            />
          description: Optional parameters.
        - id: userDelegationKey
          type: <xref uid="@azure/storage-file-datalake.UserDelegationKey" />
          description: Return value of `blobServiceClient.getUserDelegationKey()`
      return:
        description: >-
          The SAS URI consisting of the URI to the resource represented by this
          client, followed by the generated SAS token.
        type: Promise&lt;string&gt;
  - name: query(string, FileQueryOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.query'
    package: '@azure/storage-file-datalake'
    summary: >-
      Quick query for a JSON or CSV formatted file.


      Example usage (Node.js):


      ```ts snippet:ReadmeSampleQueryFile_Node

      import { DataLakeServiceClient } from "@azure/storage-file-datalake";


      const account = "<account>";

      const sas = "<sas token>";

      const datalakeServiceClient = new DataLakeServiceClient(
        `https://${account}.dfs.core.windows.net${sas}`,
      );


      const fileSystemName = "<file system name>";

      const fileName = "<file name>";

      const fileSystemClient =
      datalakeServiceClient.getFileSystemClient(fileSystemName);

      const fileClient = fileSystemClient.getFileClient(fileName);


      // Query and convert a file to a string

      const queryResponse = await fileClient.query("select * from BlobStorage");

      if (queryResponse.readableStreamBody) {
        const responseBuffer = await streamToBuffer(queryResponse.readableStreamBody);
        const downloaded = responseBuffer.toString();
        console.log(`Query file content: ${downloaded}`);
      }


      async function streamToBuffer(readableStream: NodeJS.ReadableStream):
      Promise<Buffer> {
        return new Promise((resolve, reject) => {
          const chunks: Buffer[] = [];
          readableStream.on("data", (data) => {
            chunks.push(data instanceof Buffer ? data : Buffer.from(data));
          });
          readableStream.on("end", () => {
            resolve(Buffer.concat(chunks));
          });
          readableStream.on("error", reject);
        });
      }

      ```
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function query(query: string, options?: FileQueryOptions):
        Promise<FileReadResponse>
      parameters:
        - id: query
          type: string
          description: ''
        - id: options
          type: <xref uid="@azure/storage-file-datalake.FileQueryOptions" />
          description: ''
      return:
        description: ''
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.FileReadResponse"
          />&gt;
  - name: read(number, number, FileReadOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.read'
    package: '@azure/storage-file-datalake'
    summary: >-
      Downloads a file from the service, including its metadata and properties.


      * In Node.js, data returns in a Readable stream readableStreamBody

      * In browsers, data returns in a promise contentAsBlob


      See https://learn.microsoft.com/rest/api/storageservices/get-blob


      * Example usage (Node.js):


      ```ts snippet:ReadmeSampleDownloadFile_Node

      import { DataLakeServiceClient } from "@azure/storage-file-datalake";

      import { DefaultAzureCredential } from "@azure/identity";


      const account = "<account>";

      const datalakeServiceClient = new DataLakeServiceClient(
        `https://${account}.dfs.core.windows.net`,
        new DefaultAzureCredential(),
      );


      const fileSystemName = "<file system name>";

      const fileName = "<file name>";

      const fileSystemClient =
      datalakeServiceClient.getFileSystemClient(fileSystemName);

      const fileClient = fileSystemClient.getFileClient(fileName);


      // Get file content from position 0 to the end

      // In Node.js, get downloaded data by accessing
      downloadResponse.readableStreamBody

      const downloadResponse = await fileClient.read();

      if (downloadResponse.readableStreamBody) {
        const downloaded = await streamToBuffer(downloadResponse.readableStreamBody);
        console.log("Downloaded file content:", downloaded.toString());
      }


      // [Node.js only] A helper method used to read a Node.js readable stream
      into a Buffer.

      async function streamToBuffer(readableStream: NodeJS.ReadableStream):
      Promise<Buffer> {
        return new Promise((resolve, reject) => {
          const chunks: Buffer[] = [];
          readableStream.on("data", (data) => {
            chunks.push(data instanceof Buffer ? data : Buffer.from(data));
          });
          readableStream.on("end", () => {
            resolve(Buffer.concat(chunks));
          });
          readableStream.on("error", reject);
        });
      }

      ```


      Example usage (browser):


      ```ts snippet:ReadmeSampleDownloadFile_Browser

      import { DataLakeServiceClient } from "@azure/storage-file-datalake";


      const account = "<account>";

      const sas = "<sas token>";

      const datalakeServiceClient = new DataLakeServiceClient(
        `https://${account}.dfs.core.windows.net${sas}`,
      );


      const fileSystemName = "<file system name>";

      const fileName = "<file name>";

      const fileSystemClient =
      datalakeServiceClient.getFileSystemClient(fileSystemName);

      const fileClient = fileSystemClient.getFileClient(fileName);


      // Get file content from position 0 to the end

      // In browsers, get downloaded data by accessing
      downloadResponse.contentAsBlob

      const downloadResponse = await fileClient.read();

      if (downloadResponse.contentAsBlob) {
        const blob = await downloadResponse.contentAsBlob;
        const downloaded = await blob.text();
        console.log(`Downloaded file content ${downloaded}`);
      }

      ```
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function read(offset?: number, count?: number, options?:
        FileReadOptions): Promise<FileReadResponse>
      parameters:
        - id: offset
          type: number
          description: Optional. Offset to read file, default value is 0.
        - id: count
          type: number
          description: >-
            Optional. How many bytes to read, default will read from offset to
            the end.
        - id: options
          type: <xref uid="@azure/storage-file-datalake.FileReadOptions" />
          description: Optional. Options when reading file.
      return:
        description: ''
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.FileReadResponse"
          />&gt;
  - name: readToBuffer(Buffer, number, number, FileReadToBufferOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.readToBuffer'
    package: '@azure/storage-file-datalake'
    summary: >-
      ONLY AVAILABLE IN NODE.JS RUNTIME.


      Reads a Data Lake file in parallel to a buffer.

      Offset and count are optional, pass 0 for both to read the entire file.


      Warning: Buffers can only support files up to about one gigabyte on 32-bit
      systems or about two

      gigabytes on 64-bit systems due to limitations of Node.js/V8. For files
      larger than this size,

      consider
      [readToFile](xref:@azure/storage-file-datalake.DataLakeFileClient.readToFile).
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function readToBuffer(buffer: Buffer, offset?: number, count?: number,
        options?: FileReadToBufferOptions): Promise<Buffer>
      parameters:
        - id: buffer
          type: Buffer
          description: Buffer to be fill, must have length larger than count
        - id: offset
          type: number
          description: From which position of the Data Lake file to read
        - id: count
          type: number
          description: >-
            How much data to be read. Will read to the end when passing
            undefined
        - id: options
          type: <xref uid="@azure/storage-file-datalake.FileReadToBufferOptions" />
          description: ''
      return:
        description: ''
        type: Promise&lt;Buffer&gt;
  - name: readToBuffer(number, number, FileReadToBufferOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.readToBuffer_1'
    package: '@azure/storage-file-datalake'
    summary: >-
      ONLY AVAILABLE IN NODE.JS RUNTIME


      Reads a Data Lake file in parallel to a buffer.

      Offset and count are optional, pass 0 for both to read the entire file


      Warning: Buffers can only support files up to about one gigabyte on 32-bit
      systems or about two

      gigabytes on 64-bit systems due to limitations of Node.js/V8. For files
      larger than this size,

      consider
      [readToFile](xref:@azure/storage-file-datalake.DataLakeFileClient.readToFile).
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function readToBuffer(offset?: number, count?: number, options?:
        FileReadToBufferOptions): Promise<Buffer>
      parameters:
        - id: offset
          type: number
          description: From which position of the Data Lake file to read(in bytes)
        - id: count
          type: number
          description: >-
            How much data(in bytes) to be read. Will read to the end when
            passing undefined
        - id: options
          type: <xref uid="@azure/storage-file-datalake.FileReadToBufferOptions" />
          description: ''
      return:
        description: ''
        type: Promise&lt;Buffer&gt;
  - name: readToFile(string, number, number, FileReadOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.readToFile'
    package: '@azure/storage-file-datalake'
    summary: >-
      ONLY AVAILABLE IN NODE.JS RUNTIME.


      Downloads a Data Lake file to a local file.

      Fails if the the given file path already exits.

      Offset and count are optional, pass 0 and undefined respectively to
      download the entire file.
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function readToFile(filePath: string, offset?: number, count?: number,
        options?: FileReadOptions): Promise<FileReadResponse>
      parameters:
        - id: filePath
          type: string
          description: ''
        - id: offset
          type: number
          description: From which position of the file to download.
        - id: count
          type: number
          description: >-
            How much data to be downloaded. Will download to the end when
            passing undefined.
        - id: options
          type: <xref uid="@azure/storage-file-datalake.FileReadOptions" />
          description: Options to read Data Lake file.
      return:
        description: |-
          The response data for file read operation,
                                               but with readableStreamBody set to undefined since its
                                               content is already read and written into a local file
                                               at the specified path.
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.FileReadResponse"
          />&gt;
  - name: setExpiry(PathExpiryOptions, FileSetExpiryOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.setExpiry'
    package: '@azure/storage-file-datalake'
    summary: Sets an expiry time on a file, once that time is met the file is deleted.
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function setExpiry(mode: PathExpiryOptions, options?:
        FileSetExpiryOptions): Promise<FileSetExpiryResponse>
      parameters:
        - id: mode
          type: <xref uid="@azure/storage-file-datalake.FileExpiryMode" />
          description: ''
        - id: options
          type: <xref uid="@azure/storage-file-datalake.FileSetExpiryOptions" />
          description: ''
      return:
        description: ''
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.FileSetExpiryResponse" />&gt;
  - name: >-
      upload(Blob | ArrayBuffer | ArrayBufferView | Buffer,
      FileParallelUploadOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.upload'
    package: '@azure/storage-file-datalake'
    summary: Uploads a Buffer(Node.js)/Blob/ArrayBuffer/ArrayBufferView to a File.
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function upload(data: Blob | ArrayBuffer | ArrayBufferView | Buffer,
        options?: FileParallelUploadOptions): Promise<FileUploadResponse>
      parameters:
        - id: data
          type: Blob | ArrayBuffer | ArrayBufferView | Buffer
          description: Buffer(Node), Blob, ArrayBuffer or ArrayBufferView
        - id: options
          type: >-
            <xref uid="@azure/storage-file-datalake.FileParallelUploadOptions"
            />
          description: ''
      return:
        description: ''
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.FileUploadResponse"
          />&gt;
  - name: uploadFile(string, FileParallelUploadOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.uploadFile'
    package: '@azure/storage-file-datalake'
    summary: |-
      ONLY AVAILABLE IN NODE.JS RUNTIME.

      Uploads a local file to a Data Lake file.
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function uploadFile(filePath: string, options?:
        FileParallelUploadOptions): Promise<FileUploadResponse>
      parameters:
        - id: filePath
          type: string
          description: Full path of the local file
        - id: options
          type: >-
            <xref uid="@azure/storage-file-datalake.FileParallelUploadOptions"
            />
          description: ''
      return:
        description: ''
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.FileUploadResponse"
          />&gt;
  - name: uploadStream(Readable, FileParallelUploadOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.uploadStream'
    package: '@azure/storage-file-datalake'
    summary: >-
      ONLY AVAILABLE IN NODE.JS RUNTIME.


      Uploads a Node.js Readable stream into a Data Lake file.

      This method will try to create a file, then starts uploading chunk by
      chunk.

      Please make sure potential size of stream doesn't exceed
      FILE_MAX_SIZE_BYTES and

      potential number of chunks doesn't exceed BLOCK_BLOB_MAX_BLOCKS.


      PERFORMANCE IMPROVEMENT TIPS:

      * Input stream highWaterMark is better to set a same value with
      options.chunkSize
        parameter, which will avoid Buffer.concat() operations.
    remarks: ''
    isDeprecated: false
    syntax:
      content: >-
        function uploadStream(stream: Readable, options?:
        FileParallelUploadOptions): Promise<FileUploadResponse>
      parameters:
        - id: stream
          type: Readable
          description: Node.js Readable stream.
        - id: options
          type: >-
            <xref uid="@azure/storage-file-datalake.FileParallelUploadOptions"
            />
          description: ''
      return:
        description: ''
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.FileUploadResponse"
          />&gt;
extends: <xref uid="@azure/storage-file-datalake.DataLakePathClient" />

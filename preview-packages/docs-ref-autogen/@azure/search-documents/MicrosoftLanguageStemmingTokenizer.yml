### YamlMime:TSType
name: MicrosoftLanguageStemmingTokenizer
uid: '@azure/search-documents.MicrosoftLanguageStemmingTokenizer'
package: '@azure/search-documents'
summary: >-
  Divides text using language-specific rules and reduces words to their base
  forms.
fullName: MicrosoftLanguageStemmingTokenizer
remarks: ''
isDeprecated: false
type: interface
properties:
  - name: isSearchTokenizer
    uid: >-
      @azure/search-documents.MicrosoftLanguageStemmingTokenizer.isSearchTokenizer
    package: '@azure/search-documents'
    summary: >-
      A value indicating how the tokenizer is used. Set to true if used as the
      search tokenizer, set to false if used as the indexing tokenizer. Default
      is false.
    fullName: isSearchTokenizer
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'isSearchTokenizer?: boolean'
      return:
        description: ''
        type: boolean
  - name: language
    uid: '@azure/search-documents.MicrosoftLanguageStemmingTokenizer.language'
    package: '@azure/search-documents'
    summary: The language to use. The default is English.
    fullName: language
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'language?: MicrosoftStemmingTokenizerLanguage'
      return:
        description: ''
        type: >-
          <xref uid="@azure/search-documents.MicrosoftStemmingTokenizerLanguage"
          />
  - name: maxTokenLength
    uid: '@azure/search-documents.MicrosoftLanguageStemmingTokenizer.maxTokenLength'
    package: '@azure/search-documents'
    summary: >-
      The maximum token length. Tokens longer than the maximum length are split.
      Maximum token length that can be used is 300 characters. Tokens longer
      than 300 characters are first split into tokens of length 300 and then
      each of those tokens is split based on the max token length set. Default
      is 255.
    fullName: maxTokenLength
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'maxTokenLength?: number'
      return:
        description: ''
        type: number
  - name: odatatype
    uid: '@azure/search-documents.MicrosoftLanguageStemmingTokenizer.odatatype'
    package: '@azure/search-documents'
    summary: >-
      Polymorphic discriminator, which specifies the different types this object
      can be
    fullName: odatatype
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'odatatype: "#Microsoft.Azure.Search.MicrosoftLanguageStemmingTokenizer"'
      return:
        description: ''
        type: >-
          "#<xref
          uid="Microsoft.Azure.Search.MicrosoftLanguageStemmingTokenizer" />"
inheritedProperties:
  - name: name
    uid: '@azure/search-documents.MicrosoftLanguageStemmingTokenizer.name'
    package: '@azure/search-documents'
    summary: >-
      The name of the tokenizer. It must only contain letters, digits, spaces,
      dashes or underscores, can only start and end with alphanumeric
      characters, and is limited to 128 characters.
    fullName: name
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'name: string'
      return:
        description: ''
        type: string
    inheritanceDescription: >-
      <b>Inherited From</b>
      [BaseLexicalTokenizer.name](xref:@azure/search-documents.BaseLexicalTokenizer.name)
extends: <xref uid="@azure/search-documents.BaseLexicalTokenizer" />

### YamlMime:TSType
name: CustomAnalyzer
uid: '@azure/search-documents.CustomAnalyzer'
package: '@azure/search-documents'
summary: >-
  Allows you to take control over the process of converting text into
  indexable/searchable tokens.

  It's a user-defined configuration consisting of a single predefined tokenizer
  and one or more

  filters. The tokenizer is responsible for breaking text into tokens, and the
  filters for

  modifying tokens emitted by the tokenizer.
fullName: CustomAnalyzer
remarks: ''
isDeprecated: false
type: interface
properties:
  - name: charFilters
    uid: '@azure/search-documents.CustomAnalyzer.charFilters'
    package: '@azure/search-documents'
    summary: >-
      A list of character filters used to prepare input text before it is
      processed by the

      tokenizer. For instance, they can replace certain characters or symbols.
      The filters are run

      in the order in which they are listed.
    fullName: charFilters
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'charFilters?: string[]'
      return:
        description: ''
        type: string[]
  - name: name
    uid: '@azure/search-documents.CustomAnalyzer.name'
    package: '@azure/search-documents'
    summary: >-
      The name of the analyzer. It must only contain letters, digits, spaces,
      dashes or underscores,

      can only start and end with alphanumeric characters, and is limited to 128
      characters.
    fullName: name
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'name: string'
      return:
        description: ''
        type: string
  - name: odatatype
    uid: '@azure/search-documents.CustomAnalyzer.odatatype'
    package: '@azure/search-documents'
    summary: Polymorphic Discriminator
    fullName: odatatype
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'odatatype: "#Microsoft.Azure.Search.CustomAnalyzer"'
      return:
        description: ''
        type: '"#<xref uid="Microsoft.Azure.Search.CustomAnalyzer" />"'
  - name: tokenFilters
    uid: '@azure/search-documents.CustomAnalyzer.tokenFilters'
    package: '@azure/search-documents'
    summary: >-
      A list of token filters used to filter out or modify the tokens generated
      by a tokenizer. For

      example, you can specify a lowercase filter that converts all characters
      to lowercase. The

      filters are run in the order in which they are listed.
    fullName: tokenFilters
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'tokenFilters?: string[]'
      return:
        description: ''
        type: string[]
  - name: tokenizerName
    uid: '@azure/search-documents.CustomAnalyzer.tokenizerName'
    package: '@azure/search-documents'
    summary: >-
      The name of the tokenizer to use to divide continuous text into a sequence
      of tokens, such as

      breaking a sentence into words.
      [KnownTokenizerNames](xref:@azure/search-documents.KnownTokenizerNames) is
      an enum containing built-in tokenizer names.
    fullName: tokenizerName
    remarks: ''
    isDeprecated: false
    syntax:
      content: 'tokenizerName: string'
      return:
        description: ''
        type: string
